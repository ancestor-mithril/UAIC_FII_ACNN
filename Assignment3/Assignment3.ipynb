{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "import gzip\n",
    "import pickle\n",
    "from numpy import ndarray\n",
    "import numpy as np\n",
    "from scipy.special import softmax as s_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_data() -> Tuple[Tuple[ndarray, ndarray], Tuple[ndarray, ndarray], Tuple[ndarray, ndarray]]:\n",
    "    with gzip.open(\"mnist.pkl.gz\", \"rb\") as fd:\n",
    "        train_set, valid_set, test_set = pickle.load(fd, encoding=\"latin\")\n",
    "    return train_set, valid_set, test_set\n",
    "\n",
    "\n",
    "g_train, g_valid, g_test = load_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [1, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 1, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 1, 0]])"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_one_hot_labels(labels: ndarray) -> ndarray:\n",
    "    one_hot_labels = np.full((labels.shape[0], 10), 0)\n",
    "    for i in range(labels.shape[0]):\n",
    "        one_hot_labels[i, labels[i]] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "get_one_hot_labels(g_train[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "def sigmoid(x: ndarray) -> ndarray:\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_prime(x: ndarray) -> ndarray:\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def softmax(x: ndarray) -> ndarray:\n",
    "    return s_softmax(x, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n array([1, 4, 6, ..., 0, 2, 3], dtype=int64))"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unison_shuffle(a: ndarray, b: ndarray) -> Tuple[ndarray, ndarray]:\n",
    "    \"\"\"\n",
    "\n",
    "    :return: unison shuffle using numpy advanced array indexing\n",
    "    \"\"\"\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "unison_shuffle(g_train[0], g_train[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "def blur_image(image: ndarray) -> ndarray:\n",
    "    \"\"\"\n",
    "\n",
    "    :param image: ndarray(28, 28)\n",
    "    :return: box-blurred image\n",
    "    \"\"\"\n",
    "    copy = np.array(image, copy=True)\n",
    "    dx = [-1, -1, -1, 0, 1, 1, 1, 0]\n",
    "    dy = [-1, 0, 1, 1, 1, 0, -1, -1]\n",
    "    for i in range(copy.shape[0]):\n",
    "        for j in range(copy.shape[1]):\n",
    "            s = copy[i, j] * 8\n",
    "            for ii, jj in zip(dx, dy):\n",
    "                x = i + ii\n",
    "                y = j + jj\n",
    "                if 0 <= x < copy.shape[0] and 0 <= y < copy.shape[1]:\n",
    "                    s += copy[x, y]\n",
    "            s /= 16\n",
    "            copy[i, j] = s\n",
    "    return copy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "def augment_data(data: Tuple[ndarray, ndarray]) -> Tuple[ndarray, ndarray]:\n",
    "    labels = data[1]\n",
    "    data = data[0]\n",
    "    blurred_set = np.array([blur_image(image.reshape([28, 28])).reshape((784, )) for image in data])\n",
    "    return np.concatenate((data, blurred_set), axis=0), np.concatenate((labels, labels), axis=0)\n",
    "\n",
    "# TODO: use some library to rotate or flip the images\n",
    "# tf.image.flip_left_right\n",
    "# tf.image.random_brightness\n",
    "# data_augmentation = tf.keras.Sequential([\n",
    "#      layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "#      layers.experimental.preprocessing.RandomRotation(0.2)])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 9.1% on validation\n",
      "Iteration 1: 90.94% on validation\n",
      "Iteration 2: 90.91% on validation\n",
      "Iteration 3: 90.86% on validation\n",
      "Iteration 4: 90.84% on validation\n",
      "Iteration 5: 90.83% on validation\n",
      "Iteration 6: 90.75% on validation\n"
     ]
    }
   ],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, layers: List[int]):\n",
    "        self.layers = layers\n",
    "        self.weights = self.init_weights()\n",
    "\n",
    "        self.iterations = 10\n",
    "        self.learning_rate = 0.005\n",
    "        self.mini_batch_size = 200\n",
    "        self.dropout = False\n",
    "        self.dropout_rate = 0.5\n",
    "        self.augment = False\n",
    "        self.activations = [sigmoid, softmax]\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        initialization is done by generating random numbers with the mean=loc=0, and sd=scale=sqrt(weights\n",
    "        that enter the neuron), and size is a matrix having all weights for each neuron on the same layer\n",
    "\n",
    "        :return:a list of matrices of weights for each intermediary layer\n",
    "        \"\"\"\n",
    "        return [\n",
    "            np.random.normal(loc=0, scale=(1 / np.sqrt(self.layers[i])), size=(self.layers[i] + 1, self.layers[i + 1]))\n",
    "            for i in range(len(self.layers) - 1)\n",
    "        ]\n",
    "\n",
    "    def init_args(self, **kwargs):\n",
    "        if 'iterations' in kwargs:\n",
    "            self.iterations = kwargs['iterations']\n",
    "        if 'learning_rate' in kwargs:\n",
    "            self.learning_rate = kwargs['learning_rate']\n",
    "        if 'mini_batch_size' in kwargs:\n",
    "            self.mini_batch_size = kwargs['mini_batch_size']\n",
    "        if 'dropout' in kwargs:\n",
    "            self.dropout = kwargs['dropout']\n",
    "        if 'dropout_rate' in kwargs:\n",
    "            self.dropout_rate = kwargs['dropout_rate']\n",
    "        if 'augment' in kwargs:\n",
    "            self.augment = kwargs['augment']\n",
    "        if 'activations' in kwargs:\n",
    "            self.activations = kwargs['activations']\n",
    "\n",
    "        # TODO: also add L2 and momentum\n",
    "        # TODO: try a version with RMSProp\n",
    "\n",
    "    def train(self, training_set: Tuple[ndarray, ndarray], validation_set: ndarray = None, **kwargs):\n",
    "        self.init_args(**kwargs)\n",
    "        if validation_set is None:\n",
    "            validation_set = training_set\n",
    "        if self.augment:\n",
    "            training_set = augment_data(training_set)\n",
    "        data = training_set[0]\n",
    "        labels = get_one_hot_labels(training_set[1])\n",
    "        for i in range(self.iterations):\n",
    "            print(f\"Iteration {i}: {round(self.evaluate(validation_set), 2)}% on validation\")\n",
    "            data, labels = unison_shuffle(data, labels)\n",
    "            mini_batches = [\n",
    "                (data[mini_batch:mini_batch + self.mini_batch_size],\n",
    "                 labels[mini_batch: mini_batch + self.mini_batch_size])\n",
    "                for mini_batch in range(0, labels.shape[0], self.mini_batch_size)\n",
    "            ]\n",
    "            for data, labels in mini_batches:\n",
    "                self.train_mini_batch(data, labels)\n",
    "\n",
    "\n",
    "    def train_mini_batch(self, data: ndarray, labels: ndarray) -> None:\n",
    "        \"\"\"\n",
    "        feeds forward and saves activations and raw values\n",
    "        then back propagates the error and adjusts weights\n",
    "\n",
    "        :param data: images\n",
    "        :param labels: labels\n",
    "        \"\"\"\n",
    "        raw_values, activated_values = self.feed_forward(data)\n",
    "        delta_weights = self.back_prop(labels, raw_values, activated_values)\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * delta_weights[i]\n",
    "\n",
    "    def feed_forward(self, data: ndarray, evaluate: bool = False) -> Tuple[List[ndarray], List[ndarray]]:\n",
    "        \"\"\"\n",
    "\n",
    "        :param data: images\n",
    "        :param evaluate: if true than no fancy operations are done\n",
    "        :return: raw and activated values\n",
    "        \"\"\"\n",
    "        raw_values = []\n",
    "        activated_values = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            bias = np.ones((data.shape[0], 1))\n",
    "            data = np.c_[bias, data]\n",
    "            activated_values.append(data)\n",
    "\n",
    "            z = np.dot(data, self.weights[i])\n",
    "\n",
    "            data = self.activations[i](z)\n",
    "\n",
    "            if not evaluate and self.dropout and i != len(self.layers) - 2:\n",
    "                dropout_arr = np.random.choice([0, 1], z.shape, p=[self.dropout_rate, 1 - self.dropout_rate])\n",
    "                z = np.multiply(dropout_arr * z, 1 / (1 - self.dropout_rate))\n",
    "\n",
    "            raw_values.append(z)\n",
    "\n",
    "\n",
    "        activated_values.append(data)\n",
    "        return raw_values, activated_values\n",
    "\n",
    "    def back_prop(self, labels: ndarray, raw_values: List[ndarray], activated_values: List[ndarray]) -> List[ndarray]:\n",
    "        \"\"\"\n",
    "\n",
    "        :param labels: the correct labels\n",
    "        :param raw_values: the raw values predicted by the model on each layer\n",
    "        :param activated_values: the activation of the said raw values\n",
    "        :return: a list of modifications to be done to the weights\n",
    "        \"\"\"\n",
    "        delta_weights = [0 for _ in self.weights]\n",
    "        delta = activated_values[-1] - labels\n",
    "        delta_weights[-1] = np.dot(activated_values[-2].T, delta)\n",
    "        for layer in range(2, len(self.layers)):\n",
    "            z = sigmoid_prime(raw_values[- layer]).T\n",
    "            delta = np.delete(np.dot(self.weights[- layer + 1], delta.T), 0, 0) * z  # do we need to eliminate row or column??\n",
    "            delta_weights[- layer] = np.dot(activated_values[-layer - 1].T, delta.T)\n",
    "        return delta_weights\n",
    "\n",
    "\n",
    "    def evaluate(self, data: Tuple[ndarray, ndarray]) -> float:\n",
    "        predictions_ok = 0\n",
    "        for i in range(data[0].shape[0]):\n",
    "            raw_values, activated_values = self.feed_forward(np.array([data[0][i]]), evaluate = True)\n",
    "            # raw_values = raw_values[-1]\n",
    "            activated_values = activated_values[-1]\n",
    "            if np.argmax(activated_values) == data[1][i]:\n",
    "                predictions_ok += 1\n",
    "        return predictions_ok / data[0].shape[0] * 100\n",
    "\n",
    "x = Network([784, 100, 10])\n",
    "x.train(g_train, iterations=50, dropout=True, augment=False)\n",
    "x.evaluate(g_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}